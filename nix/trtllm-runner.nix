# TensorRT-LLM Runner Library
# Provides reusable functions for building TRT-LLM based model runners
#
# Usage:
#   trtllm = callPackage ./trtllm-runner.nix { };
#   myRunner = trtllm.mkRunner { model = "nvidia/Phi-4-NVFP4"; name = "phi4"; };
#   myServer = trtllm.mkTritonServer { model = "nvidia/Phi-4-NVFP4"; name = "phi4"; };
{
  lib,
  writeShellApplication,
  writeTextFile,
  runCommand,
  python312,
  openmpi,
  tritonserver-trtllm,
  cuda,
}:

let
  python = python312;
  triton = tritonserver-trtllm;

  # Shared environment setup - the single source of truth
  envSetup = ''
    export PYTHONPATH="${triton}/python''${PYTHONPATH:+:$PYTHONPATH}"
    export LD_LIBRARY_PATH="/run/opengl-driver/lib:${triton}/lib:${triton}/python/tensorrt_llm/libs:${cuda}/lib64:${openmpi}/lib:${python}/lib''${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}"
    export CUDA_HOME="${cuda}"
  '';

  # MPI wrapper for TRT-LLM (handles multi-GPU, etc.)
  mpiExec = cmd: ''
    exec mpirun -np 1 --oversubscribe --allow-run-as-root ${cmd}
  '';

in
{
  # Re-export for consumers
  inherit python openmpi triton cuda envSetup;

  # ============================================================================
  # mkRunner: Create a CLI runner for a TRT-LLM model
  # ============================================================================
  mkRunner =
    {
      # Required
      name,           # e.g. "phi4-nvfp4"
      model,          # HuggingFace model ID, e.g. "nvidia/Phi-4-reasoning-plus-NVFP4"
      
      # Optional customization
      description ? "Run ${model} with TensorRT-LLM",
      defaultPrompt ? "Hello, my name is",
      defaultMaxTokens ? 256,
      defaultTemperature ? 0.8,
      defaultTopP ? 0.95,
      tensorParallelSize ? 1,
      chatTemplate ? null,  # Function: prompt -> formatted prompt
      extraPythonCode ? "", # Extra Python code inserted after imports
    }:
    let
      # Generate the Python script
      script = writeTextFile {
        name = "${name}.py";
        text = ''
#!/usr/bin/env python3
"""
${model} inference using TensorRT-LLM high-level API.
Auto-generated by trtllm-runner.nix
"""
import argparse
import sys

${extraPythonCode}

def main():
    parser = argparse.ArgumentParser(description="${description}")
    parser.add_argument("prompt", nargs="?", default="${defaultPrompt}",
                        help="Input prompt")
    parser.add_argument("--max-tokens", "-n", type=int, default=${toString defaultMaxTokens},
                        help="Maximum tokens to generate")
    parser.add_argument("--temperature", "-t", type=float, default=${toString defaultTemperature},
                        help="Sampling temperature")
    parser.add_argument("--top-p", type=float, default=${toString defaultTopP},
                        help="Top-p sampling")
    parser.add_argument("--tp", type=int, default=${toString tensorParallelSize},
                        help="Tensor parallelism (for multi-GPU)")
    ${if chatTemplate != null then ''parser.add_argument("--thinking", action="store_true", help="Enable thinking/reasoning mode")'' else ""}
    args = parser.parse_args()

    from tensorrt_llm import LLM, SamplingParams

    print(f"Loading ${model}...", file=sys.stderr)

    llm = LLM(
        model="${model}",
        tensor_parallel_size=args.tp,
    )

    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
    )

    prompt = args.prompt
    ${if chatTemplate != null then "prompt = format_prompt(prompt, getattr(args, 'thinking', False))" else "# No chat template"}

    outputs = llm.generate([prompt], sampling_params)

    for output in outputs:
        generated = output.outputs[0].text
        print(generated)

if __name__ == "__main__":
    main()
'';
      };
    in
    writeShellApplication {
      inherit name;
      runtimeInputs = [ python openmpi ];
      text = ''
        ${envSetup}
        ${mpiExec "${python}/bin/python ${script} \"$@\""}
      '';
      meta = {
        inherit description;
        mainProgram = name;
      };
    };

  # ============================================================================
  # mkTritonServer: Create a Triton Inference Server for a TRT-LLM model
  # ============================================================================
  mkTritonServer =
    {
      # Required
      name,           # e.g. "phi4" (server will be tritonserver-${name})
      model,          # HuggingFace model ID
      
      # Optional
      description ? "Triton Inference Server for ${model}",
      httpPort ? 8000,
      grpcPort ? 8001,
      metricsPort ? 8002,
      maxBatchSize ? 8,
      defaultMaxTokens ? 256,
      defaultTemperature ? 0.8,
      defaultTopP ? 0.95,
      tensorParallelSize ? 1,
      extraInputs ? [],     # Additional Triton inputs
      extraModelCode ? "",  # Extra Python code in model.py
      stubTimeoutSeconds ? 600,
    }:
    let
      serverName = "tritonserver-${name}";
      
      # Triton model.py using Python backend
      modelPy = writeTextFile {
        name = "model.py";
        text = ''
import json
import numpy as np
import triton_python_backend_utils as pb_utils

class TritonPythonModel:
    """${model} via TensorRT-LLM LLM API.
    
    Engine cached at: ~/.cache/tensorrt_llm/
    Auto-generated by trtllm-runner.nix
    """

    def initialize(self, args):
        self.model_config = json.loads(args["model_config"])
        params = self.model_config.get("parameters", {})
        
        model_name = params.get("model", {}).get("string_value", "${model}")
        tp_size = int(params.get("tensor_parallel_size", {}).get("string_value", "${toString tensorParallelSize}"))
        
        from tensorrt_llm import LLM, SamplingParams
        
        print(f"[${name}] Loading: {model_name}")
        print(f"[${name}] TP size: {tp_size}")
        
        self.llm = LLM(model=model_name, tensor_parallel_size=tp_size)
        self.SamplingParams = SamplingParams
        print(f"[${name}] Ready")

    def execute(self, requests):
        responses = []
        for request in requests:
            text_input = pb_utils.get_input_tensor_by_name(request, "text_input")
            prompts = [t.decode("utf-8") for t in text_input.as_numpy().flatten()]
            
            max_tokens = self._get_scalar(request, "max_tokens", ${toString defaultMaxTokens})
            temperature = self._get_scalar(request, "temperature", ${toString defaultTemperature})
            top_p = self._get_scalar(request, "top_p", ${toString defaultTopP})
            
            ${extraModelCode}
            
            sampling_params = self.SamplingParams(
                max_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
            )
            
            outputs = self.llm.generate(prompts, sampling_params)
            results = [out.outputs[0].text for out in outputs]
            
            out_tensor = pb_utils.Tensor("text_output", np.array(results, dtype=object))
            responses.append(pb_utils.InferenceResponse([out_tensor]))
        
        return responses

    def _get_scalar(self, request, name, default):
        tensor = pb_utils.get_input_tensor_by_name(request, name)
        return tensor.as_numpy().flatten()[0] if tensor else default

    def finalize(self):
        print(f"[${name}] Shutting down")
        del self.llm
'';
      };

      # Triton config.pbtxt
      configPbtxt = writeTextFile {
        name = "config.pbtxt";
        text = ''
name: "${name}"
backend: "python"
max_batch_size: ${toString maxBatchSize}

input [
  { name: "text_input", data_type: TYPE_STRING, dims: [ -1 ] },
  { name: "max_tokens", data_type: TYPE_INT32, dims: [ 1 ], optional: true },
  { name: "temperature", data_type: TYPE_FP32, dims: [ 1 ], optional: true },
  { name: "top_p", data_type: TYPE_FP32, dims: [ 1 ], optional: true }
  ${lib.concatStringsSep "\n  " extraInputs}
]

output [
  { name: "text_output", data_type: TYPE_STRING, dims: [ -1 ] }
]

instance_group [{ count: 1, kind: KIND_GPU, gpus: [ 0 ] }]

parameters { key: "model", value: { string_value: "${model}" } }
parameters { key: "tensor_parallel_size", value: { string_value: "${toString tensorParallelSize}" } }

dynamic_batching { max_queue_delay_microseconds: 100000 }
'';
      };

    in
    writeShellApplication {
      name = serverName;
      runtimeInputs = [ python openmpi ];
      text = ''
        set -euo pipefail

        MODEL_REPO="''${XDG_RUNTIME_DIR:-/tmp}/triton-${name}-repo"
        
        rm -rf "$MODEL_REPO"
        mkdir -p "$MODEL_REPO/${name}/1"
        cp ${configPbtxt} "$MODEL_REPO/${name}/config.pbtxt"
        cp ${modelPy} "$MODEL_REPO/${name}/1/model.py"

        cat <<EOF

=== Triton: ${model} ===
Endpoints:
  Health:  http://localhost:${toString httpPort}/v2/health/ready
  Models:  http://localhost:${toString httpPort}/v2/models
  Infer:   POST http://localhost:${toString httpPort}/v2/models/${name}/infer

Example:
  curl -X POST localhost:${toString httpPort}/v2/models/${name}/infer \\
    -H "Content-Type: application/json" \\
    -d '{"inputs":[{"name":"text_input","shape":[1],"datatype":"BYTES","data":["Hello"]}]}'

EOF

        ${envSetup}
        
        ${mpiExec ''
          ${triton}/bin/tritonserver \
            --model-repository="$MODEL_REPO" \
            --backend-directory="${triton}/backends" \
            --backend-config=python,stub-timeout-seconds=${toString stubTimeoutSeconds} \
            --http-port=${toString httpPort} \
            --grpc-port=${toString grpcPort} \
            --metrics-port=${toString metricsPort} \
            --log-verbose=1 \
            "$@"
        ''}
      '';
      meta = {
        inherit description;
        mainProgram = serverName;
      };
    };

  # ============================================================================
  # mkCheck: Create a flake check that verifies a runner can at least parse
  # ============================================================================
  mkCheck =
    { runner, name }:
    runCommand "check-${name}" {
      nativeBuildInputs = [ runner ];
    } ''
      # Verify the script exists and has correct shebang
      ${runner}/bin/${name} --help > /dev/null 2>&1 || ${runner}/bin/${name} --help
      touch $out
    '';
}
