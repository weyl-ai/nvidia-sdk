# TensorRT-LLM Runner Library
# Provides reusable functions for building TRT-LLM based model runners
#
# Usage:
#   trtllm = callPackage ./trtllm-runner.nix { };
#   myRunner = trtllm.mkRunner { model = "nvidia/Phi-4-NVFP4"; name = "phi4"; };
#   myServer = trtllm.mkTritonServer { model = "nvidia/Phi-4-NVFP4"; name = "phi4"; };
#   myOpenAI = trtllm.mkOpenAIServer { model = "nvidia/Phi-4-NVFP4"; name = "phi4"; };  # OpenWebUI compatible
{
  lib,
  writeShellApplication,
  writeTextFile,
  runCommand,
  python312,
  openmpi,
  tritonserver-trtllm,
  triton-trtllm-container,
  cuda,
}:

let
  python = python312;
  triton = tritonserver-trtllm;
  container = triton-trtllm-container;

  # Shared environment setup - the single source of truth
  envSetup = ''
    export PYTHONPATH="${triton}/python''${PYTHONPATH:+:$PYTHONPATH}"
    export LD_LIBRARY_PATH="/run/opengl-driver/lib:${triton}/lib:${triton}/python/tensorrt_llm/libs:${cuda}/lib64:${openmpi}/lib:${python}/lib''${LD_LIBRARY_PATH:+:$LD_LIBRARY_PATH}"
    export CUDA_HOME="${cuda}"
  '';

  # MPI wrapper for TRT-LLM (handles multi-GPU, etc.)
  mpiExec = cmd: ''
    exec mpirun -np 1 --oversubscribe --allow-run-as-root ${cmd}
  '';

in
{
  # Re-export for consumers
  inherit python openmpi triton container cuda envSetup;

  # ============================================================================
  # mkRunner: Create a CLI runner for a TRT-LLM model
  # ============================================================================
  mkRunner =
    {
      # Required
      name,           # e.g. "phi4-nvfp4"
      model,          # HuggingFace model ID, e.g. "nvidia/Phi-4-reasoning-plus-NVFP4"
      
      # Optional customization
      description ? "Run ${model} with TensorRT-LLM",
      defaultPrompt ? "Hello, my name is",
      defaultMaxTokens ? 256,
      defaultTemperature ? 0.8,
      defaultTopP ? 0.95,
      tensorParallelSize ? 1,
      chatTemplate ? null,  # Function: prompt -> formatted prompt
      extraPythonCode ? "", # Extra Python code inserted after imports
    }:
    let
      # Generate the Python script
      script = writeTextFile {
        name = "${name}.py";
        text = ''
#!/usr/bin/env python3
"""
${model} inference using TensorRT-LLM high-level API.
Auto-generated by trtllm-runner.nix
"""
import argparse
import sys

${extraPythonCode}

def main():
    parser = argparse.ArgumentParser(description="${description}")
    parser.add_argument("prompt", nargs="?", default="${defaultPrompt}",
                        help="Input prompt")
    parser.add_argument("--max-tokens", "-n", type=int, default=${toString defaultMaxTokens},
                        help="Maximum tokens to generate")
    parser.add_argument("--temperature", "-t", type=float, default=${toString defaultTemperature},
                        help="Sampling temperature")
    parser.add_argument("--top-p", type=float, default=${toString defaultTopP},
                        help="Top-p sampling")
    parser.add_argument("--tp", type=int, default=${toString tensorParallelSize},
                        help="Tensor parallelism (for multi-GPU)")
    ${if chatTemplate != null then ''parser.add_argument("--thinking", action="store_true", help="Enable thinking/reasoning mode")'' else ""}
    args = parser.parse_args()

    from tensorrt_llm import LLM, SamplingParams

    print(f"Loading ${model}...", file=sys.stderr)

    llm = LLM(
        model="${model}",
        tensor_parallel_size=args.tp,
    )

    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
    )

    prompt = args.prompt
    ${if chatTemplate != null then "prompt = format_prompt(prompt, getattr(args, 'thinking', False))" else "# No chat template"}

    outputs = llm.generate([prompt], sampling_params)

    for output in outputs:
        generated = output.outputs[0].text
        print(generated)

if __name__ == "__main__":
    main()
'';
      };
    in
    writeShellApplication {
      inherit name;
      runtimeInputs = [ python openmpi ];
      text = ''
        ${envSetup}
        ${mpiExec "${python}/bin/python ${script} \"$@\""}
      '';
      meta = {
        inherit description;
        mainProgram = name;
      };
    };

  # ============================================================================
  # mkTritonServer: Create a Triton Inference Server for a TRT-LLM model
  # ============================================================================
  mkTritonServer =
    {
      # Required
      name,           # e.g. "phi4" (server will be tritonserver-${name})
      model,          # HuggingFace model ID
      
      # Optional
      description ? "Triton Inference Server for ${model}",
      httpPort ? 8000,
      grpcPort ? 8001,
      metricsPort ? 8002,
      maxBatchSize ? 8,
      defaultMaxTokens ? 256,
      defaultTemperature ? 0.8,
      defaultTopP ? 0.95,
      tensorParallelSize ? 1,
      extraInputs ? [],     # Additional Triton inputs
      extraModelCode ? "",  # Extra Python code in model.py
      stubTimeoutSeconds ? 600,
    }:
    let
      serverName = "tritonserver-${name}";
      
      # Triton model.py using Python backend
      modelPy = writeTextFile {
        name = "model.py";
        text = ''
import json
import numpy as np
import triton_python_backend_utils as pb_utils

class TritonPythonModel:
    """${model} via TensorRT-LLM LLM API.
    
    Engine cached at: ~/.cache/tensorrt_llm/
    Auto-generated by trtllm-runner.nix
    """

    def initialize(self, args):
        self.model_config = json.loads(args["model_config"])
        params = self.model_config.get("parameters", {})
        
        model_name = params.get("model", {}).get("string_value", "${model}")
        tp_size = int(params.get("tensor_parallel_size", {}).get("string_value", "${toString tensorParallelSize}"))
        
        from tensorrt_llm import LLM, SamplingParams
        
        print(f"[${name}] Loading: {model_name}")
        print(f"[${name}] TP size: {tp_size}")
        
        self.llm = LLM(model=model_name, tensor_parallel_size=tp_size)
        self.SamplingParams = SamplingParams
        print(f"[${name}] Ready")

    def execute(self, requests):
        responses = []
        for request in requests:
            text_input = pb_utils.get_input_tensor_by_name(request, "text_input")
            prompts = [t.decode("utf-8") for t in text_input.as_numpy().flatten()]
            
            max_tokens = self._get_scalar(request, "max_tokens", ${toString defaultMaxTokens})
            temperature = self._get_scalar(request, "temperature", ${toString defaultTemperature})
            top_p = self._get_scalar(request, "top_p", ${toString defaultTopP})
            
            ${extraModelCode}
            
            sampling_params = self.SamplingParams(
                max_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
            )
            
            outputs = self.llm.generate(prompts, sampling_params)
            results = [out.outputs[0].text for out in outputs]
            
            out_tensor = pb_utils.Tensor("text_output", np.array(results, dtype=object))
            responses.append(pb_utils.InferenceResponse([out_tensor]))
        
        return responses

    def _get_scalar(self, request, name, default):
        tensor = pb_utils.get_input_tensor_by_name(request, name)
        return tensor.as_numpy().flatten()[0] if tensor else default

    def finalize(self):
        print(f"[${name}] Shutting down")
        del self.llm
'';
      };

      # Triton config.pbtxt
      configPbtxt = writeTextFile {
        name = "config.pbtxt";
        text = ''
name: "${name}"
backend: "python"
max_batch_size: ${toString maxBatchSize}

input [
  { name: "text_input", data_type: TYPE_STRING, dims: [ -1 ] },
  { name: "max_tokens", data_type: TYPE_INT32, dims: [ 1 ], optional: true },
  { name: "temperature", data_type: TYPE_FP32, dims: [ 1 ], optional: true },
  { name: "top_p", data_type: TYPE_FP32, dims: [ 1 ], optional: true }
  ${lib.concatStringsSep "\n  " extraInputs}
]

output [
  { name: "text_output", data_type: TYPE_STRING, dims: [ -1 ] }
]

instance_group [{ count: 1, kind: KIND_GPU, gpus: [ 0 ] }]

parameters { key: "model", value: { string_value: "${model}" } }
parameters { key: "tensor_parallel_size", value: { string_value: "${toString tensorParallelSize}" } }

dynamic_batching { max_queue_delay_microseconds: 100000 }
'';
      };

    in
    writeShellApplication {
      name = serverName;
      runtimeInputs = [ python openmpi ];
      text = ''
        set -euo pipefail

        MODEL_REPO="''${XDG_RUNTIME_DIR:-/tmp}/triton-${name}-repo"
        
        rm -rf "$MODEL_REPO"
        mkdir -p "$MODEL_REPO/${name}/1"
        cp ${configPbtxt} "$MODEL_REPO/${name}/config.pbtxt"
        cp ${modelPy} "$MODEL_REPO/${name}/1/model.py"

        cat <<EOF

=== Triton: ${model} ===
Endpoints:
  Health:  http://localhost:${toString httpPort}/v2/health/ready
  Models:  http://localhost:${toString httpPort}/v2/models
  Infer:   POST http://localhost:${toString httpPort}/v2/models/${name}/infer

Example:
  curl -X POST localhost:${toString httpPort}/v2/models/${name}/infer \\
    -H "Content-Type: application/json" \\
    -d '{"inputs":[{"name":"text_input","shape":[1],"datatype":"BYTES","data":["Hello"]}]}'

EOF

        ${envSetup}
        
        ${mpiExec ''
          ${triton}/bin/tritonserver \
            --model-repository="$MODEL_REPO" \
            --backend-directory="${triton}/backends" \
            --backend-config=python,stub-timeout-seconds=${toString stubTimeoutSeconds} \
            --http-port=${toString httpPort} \
            --grpc-port=${toString grpcPort} \
            --metrics-port=${toString metricsPort} \
            --log-verbose=1 \
            "$@"
        ''}
      '';
      meta = {
        inherit description;
        mainProgram = serverName;
      };
    };

  # ============================================================================
  # mkOpenAIServer: OpenAI-compatible server with streaming (for OpenWebUI)
  # Uses the Python backend with our custom model that wraps TRT-LLM LLM API
  # Plus the Triton OpenAI frontend for /v1/chat/completions
  # ============================================================================
  mkOpenAIServer =
    {
      # Required
      name,           # e.g. "phi4" (server will be openai-${name})
      model,          # HuggingFace model ID
      
      # Optional
      description ? "OpenAI-compatible server for ${model}",
      openaiPort ? 8000,        # OpenAI API port (/v1/chat/completions, etc.)
      tritonHttpPort ? 8080,    # Triton HTTP port (for native /v2/ API)
      tritonGrpcPort ? 8081,    # Triton gRPC port
      metricsPort ? 8082,
      defaultMaxTokens ? 256,
      defaultTemperature ? 0.7,
      defaultTopP ? 0.9,
      tensorParallelSize ? 1,
    }:
    let
      serverName = "openai-${name}";
      
      # Python model that wraps TRT-LLM LLM API with streaming support
      modelPy = writeTextFile {
        name = "model.py";
        text = ''
"""
${model} via TensorRT-LLM LLM API with streaming support.
Auto-generated by trtllm-runner.nix for OpenAI compatibility.
"""
import json
import numpy as np
import triton_python_backend_utils as pb_utils

class TritonPythonModel:
    def initialize(self, args):
        self.model_config = json.loads(args["model_config"])
        
        from tensorrt_llm import LLM, SamplingParams
        
        print(f"[${name}] Loading: ${model}")
        self.llm = LLM(model="${model}", tensor_parallel_size=${toString tensorParallelSize})
        self.SamplingParams = SamplingParams
        print(f"[${name}] Ready for inference")

    def execute(self, requests):
        responses = []
        for request in requests:
            text_input = pb_utils.get_input_tensor_by_name(request, "text_input")
            prompts = [t.decode("utf-8") for t in text_input.as_numpy().flatten()]
            
            max_tokens = self._get_scalar(request, "max_tokens", ${toString defaultMaxTokens})
            temperature = self._get_scalar(request, "temperature", ${toString defaultTemperature})
            top_p = self._get_scalar(request, "top_p", ${toString defaultTopP})
            
            sampling_params = self.SamplingParams(
                max_tokens=int(max_tokens),
                temperature=float(temperature),
                top_p=float(top_p),
            )
            
            outputs = self.llm.generate(prompts, sampling_params)
            results = [out.outputs[0].text for out in outputs]
            
            out_tensor = pb_utils.Tensor("text_output", np.array(results, dtype=object))
            responses.append(pb_utils.InferenceResponse([out_tensor]))
        
        return responses

    def _get_scalar(self, request, name, default):
        tensor = pb_utils.get_input_tensor_by_name(request, name)
        return tensor.as_numpy().flatten()[0] if tensor else default

    def finalize(self):
        print(f"[${name}] Shutting down")
        del self.llm
'';
      };

      # Triton config.pbtxt - using Python backend
      configPbtxt = writeTextFile {
        name = "config.pbtxt";
        text = ''
name: "${name}"
backend: "python"
max_batch_size: 8

input [
  { name: "text_input", data_type: TYPE_STRING, dims: [ -1 ] },
  { name: "max_tokens", data_type: TYPE_INT32, dims: [ 1 ], optional: true },
  { name: "temperature", data_type: TYPE_FP32, dims: [ 1 ], optional: true },
  { name: "top_p", data_type: TYPE_FP32, dims: [ 1 ], optional: true }
]

output [
  { name: "text_output", data_type: TYPE_STRING, dims: [ -1 ] }
]

instance_group [{ count: 1, kind: KIND_GPU, gpus: [ 0 ] }]

dynamic_batching { max_queue_delay_microseconds: 100000 }
'';
      };

    in
    writeShellApplication {
      name = serverName;
      runtimeInputs = [ python openmpi ];
      text = ''
        set -euo pipefail

        MODEL_REPO="''${XDG_RUNTIME_DIR:-/tmp}/openai-${name}-repo"
        
        rm -rf "$MODEL_REPO"
        mkdir -p "$MODEL_REPO/${name}/1"
        cp ${configPbtxt} "$MODEL_REPO/${name}/config.pbtxt"
        cp ${modelPy} "$MODEL_REPO/${name}/1/model.py"

        cat <<EOF

╔══════════════════════════════════════════════════════════════════╗
║  OpenAI-Compatible Server: ${name}
║  Model: ${model}
╠══════════════════════════════════════════════════════════════════╣
║  OpenAI API:
║    http://localhost:${toString openaiPort}/v1/chat/completions
║    http://localhost:${toString openaiPort}/v1/completions  
║    http://localhost:${toString openaiPort}/v1/models
║
║  Triton Native API:
║    http://localhost:${toString tritonHttpPort}/v2/models/${name}/infer
╠══════════════════════════════════════════════════════════════════╣
║  OpenWebUI Configuration:
║    Provider: OpenAI
║    Base URL: http://localhost:${toString openaiPort}/v1
║    API Key:  any-value-works
╠══════════════════════════════════════════════════════════════════╣
║  Test:
║    curl -s http://localhost:${toString openaiPort}/v1/chat/completions \\
║      -H "Content-Type: application/json" \\
║      -d '{"model":"${name}","messages":[{"role":"user","content":"Hello!"}]}'
╚══════════════════════════════════════════════════════════════════╝

EOF

        ${envSetup}
        
        # Add openai_frontend to path
        export PYTHONPATH="${triton}/python/openai/openai_frontend''${PYTHONPATH:+:$PYTHONPATH}"
        
        # The tritonserver Python API defaults to /opt/tritonserver/backends
        sudo mkdir -p /opt/tritonserver 2>/dev/null || mkdir -p /opt/tritonserver 2>/dev/null || true
        if [[ ! -e /opt/tritonserver/backends ]]; then
          sudo ln -sf ${triton}/backends /opt/tritonserver/backends 2>/dev/null || \
            ln -sf ${triton}/backends /opt/tritonserver/backends 2>/dev/null || true
        fi
        
        exec ${python}/bin/python ${triton}/python/openai/openai_frontend/main.py \
          --model-repository="$MODEL_REPO" \
          --tokenizer="${model}" \
          --backend=tensorrtllm \
          --openai-port=${toString openaiPort} \
          --default-max-tokens=${toString defaultMaxTokens} \
          --enable-kserve-frontends \
          --kserve-http-port=${toString tritonHttpPort} \
          --kserve-grpc-port=${toString tritonGrpcPort} \
          "$@"
      '';
      meta = {
        inherit description;
        mainProgram = serverName;
      };
    };

  # ============================================================================
  # mkCheck: Create a flake check that verifies a runner can at least parse
  # ============================================================================
  mkCheck =
    { runner, name }:
    runCommand "check-${name}" {
      nativeBuildInputs = [ runner ];
    } ''
      # Verify the script exists and has correct shebang
      ${runner}/bin/${name} --help > /dev/null 2>&1 || ${runner}/bin/${name} --help
      touch $out
    '';
}
